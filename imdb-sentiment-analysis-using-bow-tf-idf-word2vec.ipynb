{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMDB Sentiment Analysis Using BagOfWords,TF-IDF,Word2Vec","metadata":{}},{"cell_type":"markdown","source":"IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\nThis is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. The Dataset consists of 25,000 highly polar movie reviews for training and 25,000 for testing. So, We are going to predict the number of positive and negative reviews using classification ML algorithms.\n\n<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/69/IMDB_Logo_2016.svg/640px-IMDB_Logo_2016.svg.png\"></center>","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-24T15:12:25.170094Z","iopub.execute_input":"2022-06-24T15:12:25.170624Z","iopub.status.idle":"2022-06-24T15:12:25.213487Z","shell.execute_reply.started":"2022-06-24T15:12:25.170509Z","shell.execute_reply":"2022-06-24T15:12:25.212631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:12:25.215292Z","iopub.execute_input":"2022-06-24T15:12:25.215889Z","iopub.status.idle":"2022-06-24T15:12:26.935899Z","shell.execute_reply.started":"2022-06-24T15:12:25.215854Z","shell.execute_reply":"2022-06-24T15:12:26.93411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:12:26.937729Z","iopub.execute_input":"2022-06-24T15:12:26.938106Z","iopub.status.idle":"2022-06-24T15:12:26.983434Z","shell.execute_reply.started":"2022-06-24T15:12:26.938074Z","shell.execute_reply":"2022-06-24T15:12:26.98237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now Let's convert the sentiment column values to numerical values by a simple python list comprehension expression which is given below.","metadata":{}},{"cell_type":"code","source":"df['sentiment'] = [1 if sentiment == 'positive' else 0 for sentiment in df['sentiment']]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:12:26.986174Z","iopub.execute_input":"2022-06-24T15:12:26.986555Z","iopub.status.idle":"2022-06-24T15:12:27.047161Z","shell.execute_reply.started":"2022-06-24T15:12:26.986522Z","shell.execute_reply":"2022-06-24T15:12:27.045865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initial Data Analysis","metadata":{}},{"cell_type":"code","source":"import seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:12:27.049187Z","iopub.execute_input":"2022-06-24T15:12:27.049639Z","iopub.status.idle":"2022-06-24T15:12:28.424397Z","shell.execute_reply.started":"2022-06-24T15:12:27.049595Z","shell.execute_reply":"2022-06-24T15:12:28.422903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(df['sentiment'])","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:12:28.426413Z","iopub.execute_input":"2022-06-24T15:12:28.427009Z","iopub.status.idle":"2022-06-24T15:12:28.647588Z","shell.execute_reply.started":"2022-06-24T15:12:28.42694Z","shell.execute_reply":"2022-06-24T15:12:28.645815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:12:28.650317Z","iopub.execute_input":"2022-06-24T15:12:28.6509Z","iopub.status.idle":"2022-06-24T15:12:28.661494Z","shell.execute_reply.started":"2022-06-24T15:12:28.650843Z","shell.execute_reply":"2022-06-24T15:12:28.66017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have the equal number of positive and negative reviews, we can say that the dataset is very well balanced.","metadata":{}},{"cell_type":"markdown","source":"# Text Preprocessing","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:12:28.663824Z","iopub.execute_input":"2022-06-24T15:12:28.664888Z","iopub.status.idle":"2022-06-24T15:12:29.777605Z","shell.execute_reply.started":"2022-06-24T15:12:28.664834Z","shell.execute_reply":"2022-06-24T15:12:29.776035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As this is sentiment analysis we can use Stemming instead of Lemmatization.\n\nTo know about when to use stemming and lemmatization see below-\n\n<b>Stemmming</b> = Spam classification, Review classification\n\n<b>Lemmatization</b> = Text summarization, Language translation, chatbot","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:12:29.779684Z","iopub.execute_input":"2022-06-24T15:12:29.781416Z","iopub.status.idle":"2022-06-24T15:12:29.788419Z","shell.execute_reply.started":"2022-06-24T15:12:29.781351Z","shell.execute_reply":"2022-06-24T15:12:29.78663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are going to apply stemming to all the sentences in the Dataset and we are also going to remove stopwords after converting all the words in the dataset to lowercase. After that we are going to store these sentences in a list named \"corpus\".","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\ncorpus = []\nfor i in tqdm(range(0,len(df))):\n    sentence = re.sub('[^a-zA-Z]',' ',df['review'][i]) ## each review is each sentence\n    sentence = sentence.lower() ## Lower casing the words in each sentence\n    sentence = sentence.split() ## splitting sentences to words and storing it as a list of words\n    sentence = [ps.stem(word) for word in sentence if not word in stopwords.words('english')]  ## Removing stop words and applying stemming \n    sentence = ' '.join(sentence)  ## Joining words again to form the sentences\n    corpus.append(sentence) ### storing each sentences to corpus","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:15:09.40072Z","iopub.execute_input":"2022-06-24T15:15:09.401817Z","iopub.status.idle":"2022-06-24T15:45:49.829573Z","shell.execute_reply.started":"2022-06-24T15:15:09.401741Z","shell.execute_reply":"2022-06-24T15:45:49.827976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## corpus  ## If u want to see the corpus","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:45:56.128139Z","iopub.execute_input":"2022-06-24T15:45:56.129532Z","iopub.status.idle":"2022-06-24T15:45:56.133669Z","shell.execute_reply.started":"2022-06-24T15:45:56.129496Z","shell.execute_reply":"2022-06-24T15:45:56.13247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Bag of Words Model","metadata":{}},{"cell_type":"code","source":"## Creating bag of words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=2500)\nX1 = cv.fit_transform(corpus).toarray()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:46:00.288048Z","iopub.execute_input":"2022-06-24T15:46:00.288423Z","iopub.status.idle":"2022-06-24T15:46:08.997883Z","shell.execute_reply.started":"2022-06-24T15:46:00.288393Z","shell.execute_reply":"2022-06-24T15:46:08.99643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y1 = pd.get_dummies(df['sentiment'])\ny1 = y1.iloc[:,1].values","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:46:33.203864Z","iopub.execute_input":"2022-06-24T15:46:33.204249Z","iopub.status.idle":"2022-06-24T15:46:33.213401Z","shell.execute_reply.started":"2022-06-24T15:46:33.204218Z","shell.execute_reply":"2022-06-24T15:46:33.212105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X1,y1,test_size = 0.20, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:46:38.299642Z","iopub.execute_input":"2022-06-24T15:46:38.300502Z","iopub.status.idle":"2022-06-24T15:46:39.03556Z","shell.execute_reply.started":"2022-06-24T15:46:38.300462Z","shell.execute_reply":"2022-06-24T15:46:39.034174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train1.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-24T16:41:43.509616Z","iopub.execute_input":"2022-06-24T16:41:43.509991Z","iopub.status.idle":"2022-06-24T16:41:43.518014Z","shell.execute_reply.started":"2022-06-24T16:41:43.50996Z","shell.execute_reply":"2022-06-24T16:41:43.516904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train1.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-24T16:41:57.731035Z","iopub.execute_input":"2022-06-24T16:41:57.734731Z","iopub.status.idle":"2022-06-24T16:41:57.752203Z","shell.execute_reply.started":"2022-06-24T16:41:57.734589Z","shell.execute_reply":"2022-06-24T16:41:57.750712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmodel1 = MultinomialNB().fit(X_train1,y_train1)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:46:42.096461Z","iopub.execute_input":"2022-06-24T15:46:42.096895Z","iopub.status.idle":"2022-06-24T15:46:43.711026Z","shell.execute_reply.started":"2022-06-24T15:46:42.096861Z","shell.execute_reply":"2022-06-24T15:46:43.709806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred1 = model1.predict(X_test1)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:46:57.695429Z","iopub.execute_input":"2022-06-24T15:46:57.695833Z","iopub.status.idle":"2022-06-24T15:46:57.852149Z","shell.execute_reply.started":"2022-06-24T15:46:57.695803Z","shell.execute_reply":"2022-06-24T15:46:57.850307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report\nprint(accuracy_score(y_test1,y_pred1))\nprint(classification_report(y_pred1,y_test1))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:47:12.443872Z","iopub.execute_input":"2022-06-24T15:47:12.444293Z","iopub.status.idle":"2022-06-24T15:47:12.476054Z","shell.execute_reply.started":"2022-06-24T15:47:12.444261Z","shell.execute_reply":"2022-06-24T15:47:12.474977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making a New Prediction","metadata":{}},{"cell_type":"code","source":"new_review = 'I love this movie so much. It\\'s really great'\nnew_review = re.sub('[^a-zA-Z]', ' ', new_review)\nnew_review = new_review.lower()\nnew_review = new_review.split()\nps = PorterStemmer()\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\nnew_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\nnew_review = ' '.join(new_review)\nnew_corpus = [new_review]\nnew_X_test = cv.transform(new_corpus).toarray()\nnew_y_pred = model1.predict(new_X_test)\nprint(new_y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:16:21.096917Z","iopub.execute_input":"2022-06-24T18:16:21.097603Z","iopub.status.idle":"2022-06-24T18:16:21.114639Z","shell.execute_reply.started":"2022-06-24T18:16:21.097549Z","shell.execute_reply":"2022-06-24T18:16:21.113654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. TF-IDF","metadata":{}},{"cell_type":"code","source":"## Creating tf-idf model\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntv = TfidfVectorizer(max_features=2500)\nX2 = tv.fit_transform(corpus).toarray()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:47:23.218329Z","iopub.execute_input":"2022-06-24T15:47:23.218796Z","iopub.status.idle":"2022-06-24T15:47:32.511499Z","shell.execute_reply.started":"2022-06-24T15:47:23.218746Z","shell.execute_reply":"2022-06-24T15:47:32.509877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y2=pd.get_dummies(df['sentiment'])\ny2=y2.iloc[:,1].values","metadata":{"execution":{"iopub.status.busy":"2022-06-24T16:39:46.15526Z","iopub.execute_input":"2022-06-24T16:39:46.155718Z","iopub.status.idle":"2022-06-24T16:39:46.166678Z","shell.execute_reply.started":"2022-06-24T16:39:46.155685Z","shell.execute_reply":"2022-06-24T16:39:46.165261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size=0.20, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T16:51:20.359851Z","iopub.execute_input":"2022-06-24T16:51:20.360259Z","iopub.status.idle":"2022-06-24T16:51:20.909522Z","shell.execute_reply.started":"2022-06-24T16:51:20.360225Z","shell.execute_reply":"2022-06-24T16:51:20.908241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train2.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-24T16:51:23.731754Z","iopub.execute_input":"2022-06-24T16:51:23.732787Z","iopub.status.idle":"2022-06-24T16:51:23.739973Z","shell.execute_reply.started":"2022-06-24T16:51:23.732728Z","shell.execute_reply":"2022-06-24T16:51:23.738963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train2.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-24T16:51:26.061761Z","iopub.execute_input":"2022-06-24T16:51:26.062395Z","iopub.status.idle":"2022-06-24T16:51:26.072082Z","shell.execute_reply.started":"2022-06-24T16:51:26.062342Z","shell.execute_reply":"2022-06-24T16:51:26.070867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmodel2 = MultinomialNB().fit(X_train2, y_train2)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T16:51:28.679099Z","iopub.execute_input":"2022-06-24T16:51:28.679465Z","iopub.status.idle":"2022-06-24T16:51:28.979758Z","shell.execute_reply.started":"2022-06-24T16:51:28.679435Z","shell.execute_reply":"2022-06-24T16:51:28.978429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred2 = model2.predict(X_test2)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T16:51:31.547027Z","iopub.execute_input":"2022-06-24T16:51:31.547725Z","iopub.status.idle":"2022-06-24T16:51:31.620032Z","shell.execute_reply.started":"2022-06-24T16:51:31.547675Z","shell.execute_reply":"2022-06-24T16:51:31.61854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(accuracy_score(y_test2,y_pred2))\nprint(classification_report(y_pred2,y_test2))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T16:51:33.758649Z","iopub.execute_input":"2022-06-24T16:51:33.759129Z","iopub.status.idle":"2022-06-24T16:51:33.791551Z","shell.execute_reply.started":"2022-06-24T16:51:33.759091Z","shell.execute_reply":"2022-06-24T16:51:33.79022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making a New Prediction","metadata":{}},{"cell_type":"code","source":"new_review = 'I Hate this movie so much. It\\'s ok.'\nnew_review = re.sub('[^a-zA-Z]', ' ', new_review)\nnew_review = new_review.lower()\nnew_review = new_review.split()\nps = PorterStemmer()\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\nnew_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\nnew_review = ' '.join(new_review)\nnew_corpus = [new_review]\nnew_X_test = cv.transform(new_corpus).toarray()\nnew_y_pred = model2.predict(new_X_test)\nprint(new_y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:17:36.853863Z","iopub.execute_input":"2022-06-24T18:17:36.855254Z","iopub.status.idle":"2022-06-24T18:17:36.866341Z","shell.execute_reply.started":"2022-06-24T18:17:36.855191Z","shell.execute_reply":"2022-06-24T18:17:36.865139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Word2Vec","metadata":{}},{"cell_type":"code","source":"import gensim.downloader as api\nwv = api.load('word2vec-google-news-300')","metadata":{"execution":{"iopub.status.busy":"2022-06-24T16:52:46.501162Z","iopub.execute_input":"2022-06-24T16:52:46.501641Z","iopub.status.idle":"2022-06-24T16:58:16.959466Z","shell.execute_reply.started":"2022-06-24T16:52:46.501605Z","shell.execute_reply":"2022-06-24T16:58:16.958037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:18:12.258017Z","iopub.execute_input":"2022-06-24T17:18:12.258551Z","iopub.status.idle":"2022-06-24T17:18:12.266062Z","shell.execute_reply.started":"2022-06-24T17:18:12.258513Z","shell.execute_reply":"2022-06-24T17:18:12.265013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ncorpus1 = []\nfor i in tqdm(range(0,len(df))):\n    sentence = re.sub('[^a-zA-Z]',' ',df['review'][i]) ## each review is each sentence\n    sentence = sentence.lower() ## Lower casing the words in each sentence\n    sentence = sentence.split() ## splitting sentences to words and storing it as a list of words\n    sentence = [lemmatizer.lemmatize(word) for word in sentence if not word in stopwords.words('english')]  ## Removing stop words and applying Lemmatizer \n    sentence = ' '.join(sentence)  ## Joining words again to form the sentences\n    corpus1.append(sentence) ### storing each sentences to corpus","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:18:15.826738Z","iopub.execute_input":"2022-06-24T17:18:15.827274Z","iopub.status.idle":"2022-06-24T17:46:25.399965Z","shell.execute_reply.started":"2022-06-24T17:18:15.827233Z","shell.execute_reply":"2022-06-24T17:46:25.398275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk import sent_tokenize\nfrom gensim.utils import simple_preprocess","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:47:19.870737Z","iopub.execute_input":"2022-06-24T17:47:19.871643Z","iopub.status.idle":"2022-06-24T17:47:19.880123Z","shell.execute_reply.started":"2022-06-24T17:47:19.8716Z","shell.execute_reply":"2022-06-24T17:47:19.87887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = []\nfor sent in corpus1:\n    sent_token = sent_tokenize(sent)\n    for sent in sent_token:\n        words.append(simple_preprocess(sent))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:47:25.428442Z","iopub.execute_input":"2022-06-24T17:47:25.430217Z","iopub.status.idle":"2022-06-24T17:47:53.979886Z","shell.execute_reply.started":"2022-06-24T17:47:25.430166Z","shell.execute_reply":"2022-06-24T17:47:53.97843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#words  ## to see the nested words list","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:48:04.214231Z","iopub.execute_input":"2022-06-24T17:48:04.214649Z","iopub.status.idle":"2022-06-24T17:48:04.220052Z","shell.execute_reply.started":"2022-06-24T17:48:04.214616Z","shell.execute_reply":"2022-06-24T17:48:04.21851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\nmodel3 = gensim.models.Word2Vec(words,window=5,min_count=2)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:48:08.662621Z","iopub.execute_input":"2022-06-24T17:48:08.663087Z","iopub.status.idle":"2022-06-24T17:48:59.877453Z","shell.execute_reply.started":"2022-06-24T17:48:08.66305Z","shell.execute_reply":"2022-06-24T17:48:59.876189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## model3.wv.index_to_key","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:49:23.073897Z","iopub.execute_input":"2022-06-24T17:49:23.074755Z","iopub.status.idle":"2022-06-24T17:49:23.078983Z","shell.execute_reply.started":"2022-06-24T17:49:23.074714Z","shell.execute_reply":"2022-06-24T17:49:23.077895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3.corpus_count","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:49:26.3879Z","iopub.execute_input":"2022-06-24T17:49:26.388749Z","iopub.status.idle":"2022-06-24T17:49:26.396734Z","shell.execute_reply.started":"2022-06-24T17:49:26.388705Z","shell.execute_reply":"2022-06-24T17:49:26.395291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3.epochs","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:49:29.712399Z","iopub.execute_input":"2022-06-24T17:49:29.714001Z","iopub.status.idle":"2022-06-24T17:49:29.72222Z","shell.execute_reply.started":"2022-06-24T17:49:29.713944Z","shell.execute_reply":"2022-06-24T17:49:29.720571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def avg_word2vec(doc):\n    return np.mean([model3.wv[word] for word in doc if word in model3.wv.index_to_key], axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:51:27.293891Z","iopub.execute_input":"2022-06-24T17:51:27.294356Z","iopub.status.idle":"2022-06-24T17:51:27.301068Z","shell.execute_reply.started":"2022-06-24T17:51:27.29432Z","shell.execute_reply":"2022-06-24T17:51:27.299856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X3 = []\nfor i in tqdm(range(len(words))):\n    X3.append(avg_word2vec(words[i]))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:52:54.789252Z","iopub.execute_input":"2022-06-24T17:52:54.789709Z","iopub.status.idle":"2022-06-24T18:08:21.024168Z","shell.execute_reply.started":"2022-06-24T17:52:54.789673Z","shell.execute_reply":"2022-06-24T18:08:21.022559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(X3)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:08:41.973085Z","iopub.execute_input":"2022-06-24T18:08:41.974643Z","iopub.status.idle":"2022-06-24T18:08:41.981599Z","shell.execute_reply.started":"2022-06-24T18:08:41.974582Z","shell.execute_reply":"2022-06-24T18:08:41.980786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_new = np.array(X3)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:08:45.052288Z","iopub.execute_input":"2022-06-24T18:08:45.053033Z","iopub.status.idle":"2022-06-24T18:08:45.142854Z","shell.execute_reply.started":"2022-06-24T18:08:45.052993Z","shell.execute_reply":"2022-06-24T18:08:45.141576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_new.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:08:47.187153Z","iopub.execute_input":"2022-06-24T18:08:47.187628Z","iopub.status.idle":"2022-06-24T18:08:47.195564Z","shell.execute_reply.started":"2022-06-24T18:08:47.187592Z","shell.execute_reply":"2022-06-24T18:08:47.19437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y3=pd.get_dummies(df['sentiment'])\ny3=y3.iloc[:,1].values","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:08:50.411965Z","iopub.execute_input":"2022-06-24T18:08:50.412374Z","iopub.status.idle":"2022-06-24T18:08:50.446298Z","shell.execute_reply.started":"2022-06-24T18:08:50.412339Z","shell.execute_reply":"2022-06-24T18:08:50.445208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train3, X_test3, y_train3, y_test3 = train_test_split(X_new,y3, test_size=0.20, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:08:52.973316Z","iopub.execute_input":"2022-06-24T18:08:52.974003Z","iopub.status.idle":"2022-06-24T18:08:53.010345Z","shell.execute_reply.started":"2022-06-24T18:08:52.973965Z","shell.execute_reply":"2022-06-24T18:08:53.008836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train3.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:08:55.312374Z","iopub.execute_input":"2022-06-24T18:08:55.312855Z","iopub.status.idle":"2022-06-24T18:08:55.320557Z","shell.execute_reply.started":"2022-06-24T18:08:55.312817Z","shell.execute_reply":"2022-06-24T18:08:55.319584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train3.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:08:57.392669Z","iopub.execute_input":"2022-06-24T18:08:57.393389Z","iopub.status.idle":"2022-06-24T18:08:57.399969Z","shell.execute_reply.started":"2022-06-24T18:08:57.39335Z","shell.execute_reply":"2022-06-24T18:08:57.398847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\nmodel4 = SVC(kernel='rbf', random_state=0).fit(X_train3, y_train3)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:11:31.692962Z","iopub.execute_input":"2022-06-24T18:11:31.693715Z","iopub.status.idle":"2022-06-24T18:14:02.484349Z","shell.execute_reply.started":"2022-06-24T18:11:31.693675Z","shell.execute_reply":"2022-06-24T18:14:02.482809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred3 = model4.predict(X_test3)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:14:33.962185Z","iopub.execute_input":"2022-06-24T18:14:33.962667Z","iopub.status.idle":"2022-06-24T18:14:55.502335Z","shell.execute_reply.started":"2022-06-24T18:14:33.962628Z","shell.execute_reply":"2022-06-24T18:14:55.500647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(accuracy_score(y_test3,y_pred3))\nprint(classification_report(y_pred3,y_test3))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:16:10.790263Z","iopub.execute_input":"2022-06-24T18:16:10.790842Z","iopub.status.idle":"2022-06-24T18:16:10.823948Z","shell.execute_reply.started":"2022-06-24T18:16:10.790799Z","shell.execute_reply":"2022-06-24T18:16:10.822574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making a New Prediction","metadata":{}},{"cell_type":"code","source":"new_review = 'The Dr.Strange MOM movie was great.'\nnew_review = re.sub('[^a-zA-Z]', ' ', new_review)\nnew_review = new_review.lower()\nnew_review = new_review.split()\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\nnew_review = [lemmatizer.lemmatize(word) for word in new_review if not word in set(all_stopwords)]\nnew_review = ' '.join(new_review)\nnew_corpus = [new_review]\n\nnew_words=[]\nfor sent in new_corpus:\n    sent_token = sent_tokenize(sent)\n    for sent in sent_token:\n        new_words.append(simple_preprocess(sent))\n        \nnew_X3 = []\nfor i in range(len(new_words)):\n    new_X3.append(avg_word2vec(new_words[i]))\n    \nnew_X = np.array(new_X3)\nnew_y_pred = model4.predict(new_X)\nprint(new_y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:28:04.194666Z","iopub.execute_input":"2022-06-24T18:28:04.196296Z","iopub.status.idle":"2022-06-24T18:28:04.213224Z","shell.execute_reply.started":"2022-06-24T18:28:04.196251Z","shell.execute_reply":"2022-06-24T18:28:04.21184Z"},"trusted":true},"execution_count":null,"outputs":[]}]}